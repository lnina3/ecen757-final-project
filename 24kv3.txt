The architecture of a storage system that needs to operate in a
production setting is complex. In addition to the actual data
persistence component, the system needs to have scalable and
robust solutions for load balancing, membership and failure
detection, failure recovery, replica synchronization, overload
handling, state transfer, concurrency and job scheduling, request
marshalling, request routing, system monitoring and alarming,
and configuration management. Describing the details of each of
the solutions is not possible, so this paper focuses on the core
distributed systems techniques used in Dynamo: partitioning,
replication, versioning, membership, failure handling and scaling.
Table 1 presents a summary of the list of techniques Dynamo uses
and their respective advantages.
4.1 System Interface
Dynamo stores objects associated with a key through a simple
interface; it exposes two operations: get() and put(). The get(key)
operation locates the object replicas associated with the key in the
storage system and returns a single object or a list of objects with
conflicting versions along with a context. The put(key, context,
object) operation determines where the replicas of the object
should be placed based on the associated key, and writes the
replicas to disk. The context encodes system metadata about the
object that is opaque to the caller and includes information such as
the version of the object. The context information is stored along
with the object so that the system can verify the validity of the
context object supplied in the put request.
Dynamo treats both the key and the object supplied by the caller
as an opaque array of bytes. It applies a MD5 hash on the key to
generate a 128-bit identifier, which is used to determine the
storage nodes that are responsible for serving the key.
4.2 Partitioning Algorithm
One of the key design requirements for Dynamo is that it must
scale incrementally. This requires a mechanism to dynamically
partition the data over the set of nodes (i.e., storage hosts) in the
system. Dynamo’s partitioning scheme relies on consistent
hashing to distribute the load across multiple storage hosts. In
consistent hashing [10], the output range of a hash function is
treated as a fixed circular space or “ring” (i.e. the largest hash
value wraps around to the smallest hash value). Each node in the
system is assigned a random value within this space which
represents its “position” on the ring. Each data item identified by
a key is assigned to a node by hashing the data item’s key to yield
its position on the ring, and then walking the ring clockwise to
find the first node with a position larger than the item’s position.
A
B
C
DE
F
G
Key K
Nodes B, C
and D store
keys in
range (A,B)
including
K.
Figure 2: Partitioning and replication of keys in Dynamo
ring.
Table 1: Summary of techniques used in Dynamo and
their advantages.
Problem Technique Advantage
Partitioning Consistent Hashing Incremental
Scalability
High Availability
for writes
Vector clocks with
reconciliation during
reads
Version size is
decoupled from
update rates.
Handling temporary
failures
Sloppy Quorum and
hinted handoff
Provides high
availability and
durability guarantee
when some of the
replicas are not
available.
Recovering from
permanent failures
Anti-entropy using
Merkle trees
Synchronizes
divergent replicas in
the background.
Membership and
failure detection
Gossip-based
membership protocol
and failure detection.
Preserves symmetry
and avoids having a
centralized registry
for storing
membership and
node liveness
information.199209
Thus, each node becomes responsible for the region in the ring
between it and its predecessor node on the ring. The principle
advantage of consistent hashing is that departure or arrival of a
node only affects its immediate neighbors and other nodes remain
unaffected.
The basic consistent hashing algorithm presents some challenges.
First, the random position assignment of each node on the ring
leads to non-uniform data and load distribution. Second, the basic
algorithm is oblivious to the heterogeneity in the performance of
nodes. To address these issues, Dynamo uses a variant of
consistent hashing (similar to the one used in [10, 20]): instead of
mapping a node to a single point in the circle, each node gets
assigned to multiple points in the ring. To this end, Dynamo uses
the concept of “virtual nodes”. A virtual node looks like a single
node in the system, but each node can be responsible for more
than one virtual node. Effectively, when a new node is added to
the system, it is assigned multiple positions (henceforth, “tokens”)
in the ring. The process of fine-tuning Dynamo’s partitioning
scheme is discussed in Section 6.
Using virtual nodes has the following advantages:
• If a node becomes unavailable (due to failures or routine
maintenance), the load handled by this node is evenly
dispersed across the remaining available nodes.
• When a node becomes available again, or a new node is
added to the system, the newly available node accepts a
roughly equivalent amount of load from each of the other
available nodes.
• The number of virtual nodes that a node is responsible can
decided based on its capacity, accounting for heterogeneity
in the physical infrastructure.
4.3 Replication
To achieve high availability and durability, Dynamo replicates its
data on multiple hosts. Each data item is replicated at N hosts,
where N is a parameter configured “per-instance”. Each key, k, is
assigned to a coordinator node (described in the previous section).
The coordinator is in charge of the replication of the data items
that fall within its range. In addition to locally storing each key
within its range, the coordinator replicates these keys at the N-1
clockwise successor nodes in the ring. This results in a system
where each node is responsible for the region of the ring between
it and its Nth predecessor. In Figure 2, node B replicates the key k
at nodes C and D in addition to storing it locally. Node D will
store the keys that fall in the ranges (A, B], (B, C], and (C, D].
The list of nodes that is responsible for storing a particular key is
called the preference list. The system is designed, as will be
explained in Section 4.8, so that every node in the system can
determine which nodes should be in this list for any particular
key. To account for node failures, preference list contains more
than N nodes. Note that with the use of virtual nodes, it is possible
that the first N successor positions for a particular key may be
owned by less than N distinct physical nodes (i.e. a node may
hold more than one of the first N positions). To address this, the
preference list for a key is constructed by skipping positions in the
ring to ensure that the list contains only distinct physical nodes.
4.4 Data Versioning
Dynamo provides eventual consistency, which allows for updates
to be propagated to all replicas asynchronously. A put() call may
return to its caller before the update has been applied at all the
replicas, which can result in scenarios where a subsequent get()
operation may return an object that does not have the latest
updates.. If there are no failures then there is a bound on the
update propagation times. However, under certain failure
scenarios (e.g., server outages or network partitions), updates may
not arrive at all replicas for an extended period of time.
There is a category of applications in Amazon’s platform that can
tolerate such inconsistencies and can be constructed to operate
under these conditions. For example, the shopping cart application
requires that an “Add to Cart” operation can never be forgotten or
rejected. If the most recent state of the cart is unavailable, and a
user makes changes to an older version of the cart, that change is
still meaningful and should be preserved. But at the same time it
shouldn’t supersede the currently unavailable state of the cart,
which itself may contain changes that should be preserved. Note
that both “add to cart” and “delete item from cart” operations are
translated into put requests to Dynamo. When a customer wants to
add an item to (or remove from) a shopping cart and the latest
version is not available, the item is added to (or removed from)
the older version and the divergent versions are reconciled later.
In order to provide this kind of guarantee, Dynamo treats the
result of each modification as a new and immutable version of the
data. It allows for multiple versions of an object to be present in
the system at the same time. Most of the time, new versions
subsume the previous version(s), and the system itself can
determine the authoritative version (syntactic reconciliation).
However, version branching may happen, in the presence of
failures combined with concurrent updates, resulting in
conflicting versions of an object. In these cases, the system cannot
reconcile the multiple versions of the same object and the client
must perform the reconciliation in order to collapse multiple
branches of data evolution back into one (semantic
reconciliation). A typical example of a collapse operation is
“merging” different versions of a customer’s shopping cart. Using
this reconciliation mechanism, an “add to cart” operation is never
lost. However, deleted items can resurface.
It is important to understand that certain failure modes can
potentially result in the system having not just two but several
versions of the same data. Updates in the presence of network
partitions and node failures can potentially result in an object
having distinct version sub-histories, which the system will need
to reconcile in the future. This requires us to design applications
that explicitly acknowledge the possibility of multiple versions of
the same data (in order to never lose any updates).
Dynamo uses vector clocks [12] in order to capture causality
between different versions of the same object. A vector clock is
effectively a list of (node, counter) pairs. One vector clock is
associated with every version of every object. One can determine
whether two versions of an object are on parallel branches or have
a causal ordering, by examine their vector clocks. If the counters
on the first object’s clock are less-than-or-equal to all of the nodes
in the second clock, then the first is an ancestor of the second and
can be forgotten. Otherwise, the two changes are considered to be
in conflict and require reconciliation.
In Dynamo, when a client wishes to update an object, it must
specify which version it is updating. This is done by passing the
context it obtained from an earlier read operation, which contains
the vector clock information. Upon processing a read request, if200210
Dynamo has access to multiple branches that cannot be
syntactically reconciled, it will return all the objects at the leaves,
with the corresponding version information in the context. An
update using this context is considered to have reconciled the
divergent versions and the branches are collapsed into a single
new version.
To illustrate the use of vector clocks, let us consider the example
shown in Figure 3. A client writes a new object. The node (say
Sx) that handles the write for this key increases its sequence
number and uses it to create the data's vector clock. The system
now has the object D1 and its associated clock [(Sx, 1)]. The
client updates the object. Assume the same node handles this
request as well. The system now also has object D2 and its
associated clock [(Sx, 2)]. D2 descends from D1 and therefore
over-writes D1, however there may be replicas of D1 lingering at
nodes that have not yet seen D2. Let us assume that the same
client updates the object again and a different server (say Sy)
handles the request. The system now has data D3 and its
associated clock [(Sx, 2), (Sy, 1)].
Next assume a different client reads D2 and then tries to update it,
and another node (say Sz) does the write. The system now has D4
(descendant of D2) whose version clock is [(Sx, 2), (Sz, 1)]. A
node that is aware of D1 or D2 could determine, upon receiving
D4 and its clock, that D1 and D2 are overwritten by the new data
and can be garbage collected. A node that is aware of D3 and
receives D4 will find that there is no causal relation between
them. In other words, there are changes in D3 and D4 that are not
reflected in each other. Both versions of the data must be kept and
presented to a client (upon a read) for semantic reconciliation.
Now assume some client reads both D3 and D4 (the context will
reflect that both values were found by the read). The read's
context is a summary of the clocks of D3 and D4, namely [(Sx, 2),
(Sy, 1), (Sz, 1)]. If the client performs the reconciliation and node
Sx coordinates the write, Sx will update its sequence number in
the clock. The new data D5 will have the following clock: [(Sx,
3), (Sy, 1), (Sz, 1)].
A possible issue with vector clocks is that the size of vector
clocks may grow if many servers coordinate the writes to an
object. In practice, this is not likely because the writes are usually
handled by one of the top N nodes in the preference list. In case of
network partitions or multiple server failures, write requests may
be handled by nodes that are not in the top N nodes in the
preference list causing the size of vector clock to grow. In these
scenarios, it is desirable to limit the size of vector clock. To this
end, Dynamo employs the following clock truncation scheme:
Along with each (node, counter) pair, Dynamo stores a timestamp
that indicates the last time the node updated the data item. When
the number of (node, counter) pairs in the vector clock reaches a
threshold (say 10), the oldest pair is removed from the clock.
Clearly, this truncation scheme can lead to inefficiencies in
reconciliation as the descendant relationships cannot be derived
accurately. However, this problem has not surfaced in production
and therefore this issue has not been thoroughly investigated.
4.5 Execution of get () and put () operations
Any storage node in Dynamo is eligible to receive client get and
put operations for any key. In this section, for sake of simplicity,
we describe how these operations are performed in a failure-free
environment and in the subsequent section we describe how read
and write operations are executed during failures.
Both get and put operations are invoked using Amazon’s
infrastructure-specific request processing framework over HTTP.
There are two strategies that a client can use to select a node: (1)
route its request through a generic load balancer that will select a
node based on load information, or (2) use a partition-aware client
library that routes requests directly to the appropriate coordinator
nodes. The advantage of the first approach is that the client does
not have to link any code specific to Dynamo in its application,
whereas the second strategy can achieve lower latency because it
skips a potential forwarding step.
A node handling a read or write operation is known as the
coordinator. Typically, this is the first among the top N nodes in
the preference list. If the requests are received through a load
balancer, requests to access a key may be routed to any random
node in the ring. In this scenario, the node that receives the
request will not coordinate it if the node is not in the top N of the
requested key’s preference list. Instead, that node will forward the
request to the first among the top N nodes in the preference list.
Read and write operations involve the first N healthy nodes in the
preference list, skipping over those that are down or inaccessible.
When all nodes are healthy, the top N nodes in a key’s preference
list are accessed. When there are node failures or network
partitions, nodes that are lower ranked in the preference list are
accessed.
To maintain consistency among its replicas, Dynamo uses a
consistency protocol similar to those used in quorum systems.
This protocol has two key configurable values: R and W. R is the
minimum number of nodes that must participate in a successful
read operation. W is the minimum number of nodes that must
participate in a successful write operation. Setting R and W such
that R + W > N yields a quorum-like system. In this model, the
latency of a get (or put) operation is dictated by the slowest of the
R (or W) replicas. For this reason, R and W are usually
configured to be less than N, to provide better latency.
Upon receiving a put() request for a key, the coordinator generates
the vector clock for the new version and writes the new version
locally. The coordinator then sends the new version (along with
Figure 3: Version evolution of an object over time.201211
the new vector clock) to the N highest-ranked reachable nodes. If
at least W-1 nodes respond then the write is considered
successful.
Similarly, for a get() request, the coordinator requests all existing
versions of data for that key from the N highest-ranked reachable
nodes in the preference list for that key, and then waits for R
responses before returning the result to the client. If the
coordinator ends up gathering multiple versions of the data, it
returns all the versions it deems to be causally unrelated. The
divergent versions are then reconciled and the reconciled version
superseding the current versions is written back.
4.6 Handling Failures: Hinted Handoff
If Dynamo used a traditional quorum approach it would be
unavailable during server failures and network partitions, and
would have reduced durability even under the simplest of failure
conditions. To remedy this it does not enforce strict quorum
membership and instead it uses a “sloppy quorum”; all read and
write operations are performed on the first N healthy nodes from
the preference list, which may not always be the first N nodes
encountered while walking the consistent hashing ring.
Consider the example of Dynamo configuration given in Figure 2
with N=3. In this example, if node A is temporarily down or
unreachable during a write operation then a replica that would
normally have lived on A will now be sent to node D. This is done
to maintain the desired availability and durability guarantees. The
replica sent to D will have a hint in its metadata that suggests
which node was the intended recipient of the replica (in this case
A). Nodes that receive hinted replicas will keep them in a
separate local database that is scanned periodically. Upon
detecting that A has recovered, D will attempt to deliver the
replica to A. Once the transfer succeeds, D may delete the object
from its local store without decreasing the total number of replicas
in the system.
Using hinted handoff, Dynamo ensures that the read and write
operations are not failed due to temporary node or network
failures. Applications that need the highest level of availability
can set W to 1, which ensures that a write is accepted as long as a
single node in the system has durably written the key it to its local
store. Thus, the write request is only rejected if all nodes in the
system are unavailable. However, in practice, most Amazon
services in production set a higher W to meet the desired level of
durability. A more detailed discussion of configuring N, R and W
follows in section 6.
It is imperative that a highly available storage system be capable
of handling the failure of an entire data center(s). Data center
failures happen due to power outages, cooling failures, network
failures, and natural disasters. Dynamo is configured such that
each object is replicated across multiple data centers. In essence,
the preference list of a key is constructed such that the storage
nodes are spread across multiple data centers. These datacenters
are connected through high speed network links. This scheme of
replicating across multiple datacenters allows us to handle entire
data center failures without a data outage.
4.7 Handling permanent failures: Replica
synchronization
Hinted handoff works best if the system membership churn is low
and node failures are transient. There are scenarios under which
hinted replicas become unavailable before they can be returned to
the original replica node. To handle this and other threats to
durability, Dynamo implements an anti-entropy (replica
synchronization) protocol to keep the replicas synchronized.
To detect the inconsistencies between replicas faster and to
minimize the amount of transferred data, Dynamo uses Merkle
trees [13]. A Merkle tree is a hash tree where leaves are hashes of
the values of individual keys. Parent nodes higher in the tree are
hashes of their respective children. The principal advantage of
Merkle tree is that each branch of the tree can be checked
independently without requiring nodes to download the entire tree
or the entire data set. Moreover, Merkle trees help in reducing the
amount of data that needs to be transferred while checking for
inconsistencies among replicas. For instance, if the hash values of
the root of two trees are equal, then the values of the leaf nodes in
the tree are equal and the nodes require no synchronization. If not,
it implies that the values of some replicas are different. In such
cases, the nodes may exchange the hash values of children and the
process continues until it reaches the leaves of the trees, at which
point the hosts can identify the keys that are “out of sync”. Merkle
trees minimize the amount of data that needs to be transferred for
synchronization and reduce the number of disk reads performed
during the anti-entropy process.
Dynamo uses Merkle trees for anti-entropy as follows: Each node
maintains a separate Merkle tree for each key range (the set of
keys covered by a virtual node) it hosts. This allows nodes to
compare whether the keys within a key range are up-to-date. In
this scheme, two nodes exchange the root of the Merkle tree
corresponding to the key ranges that they host in common.
Subsequently, using the tree traversal scheme described above the
nodes determine if they have any differences and perform the
appropriate synchronization action. The disadvantage with this
scheme is that many key ranges change when a node joins or
leaves the system thereby requiring the tree(s) to be recalculated.
This issue is addressed, however, by the refined partitioning
scheme described in Section 6.2.
4.8 Membership and Failure Detection
4.8.1 Ring Membership
In Amazon’s environment node outages (due to failures and
maintenance tasks) are often transient but may last for extended
intervals. A node outage rarely signifies a permanent departure
and therefore should not result in rebalancing of the partition
assignment or repair of the unreachable replicas. Similarly,
manual error could result in the unintentional startup of new
Dynamo nodes. For these reasons, it was deemed appropriate to
use an explicit mechanism to initiate the addition and removal of
nodes from a Dynamo ring. An administrator uses a command
line tool or a browser to connect to a Dynamo node and issue a
membership change to join a node to a ring or remove a node
from a ring. The node that serves the request writes the
membership change and its time of issue to persistent store. The
membership changes form a history because nodes can be
removed and added back multiple times. A gossip-based protocol
propagates membership changes and maintains an eventually
consistent view of membership. Each node contacts a peer chosen
at random every second and the two nodes efficiently reconcile
their persisted membership change histories.
When a node starts for the first time, it chooses its set of tokens
(virtual nodes in the consistent hash space) and maps nodes to
their respective token sets. The mapping is persisted on disk and202212
initially contains only the local node and token set. The mappings
stored at different Dynamo nodes are reconciled during the same
communication exchange that reconciles the membership change
histories. Therefore, partitioning and placement information also
propagates via the gossip-based protocol and each storage node is
aware of the token ranges handled by its peers. This allows each
node to forward a key’s read/write operations to the right set of
nodes directly.

Source: Giuseppe DeCandia et al., "Dynamo: Amazon’s Highly Available Key-value Store"
